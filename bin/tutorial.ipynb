{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luna Training Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook is meant to explicit every step of a training session setup and performance. However, in practice it is suitable not to gather all of those steps in a single notebook.__\n",
    "\n",
    "1) Building the network\n",
    "\n",
    "2) Setting up training environement and parameters\n",
    "\n",
    "3) Train the model\n",
    "\n",
    "4) Evaluate\n",
    "\n",
    "In the following, we demonstrate these steps based on what was proposed by [Christodoulis et al. 2018](https://arxiv.org/abs/1809.06226)\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras.losses as klosses\n",
    "import keras.optimizers as optimizers\n",
    "import keras.callbacks as kcallbacks\n",
    "import keras.initializers as initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and setup local dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "base_dir = os.path.dirname(cur_dir)\n",
    "\n",
    "import sys\n",
    "sys.path.append(base_dir)\n",
    "from utils.LungsLoader import LungsLoader\n",
    "from utils.ScanHandler import ScanHandler\n",
    "\n",
    "from src.networks.networks_utils import blocks\n",
    "from src.networks.MariaNet import MariaNet\n",
    "from src.training.config_file import ConfigFile\n",
    "from src.training.luna_training import LunaTrainer\n",
    "import src.metrics.metrics as metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "loader = LungsLoader()\n",
    "handler = ScanHandler(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First hyperparameter showing up would be the input size which is here set to 256x256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the network defined by [Christodoulis et al. 2018](https://arxiv.org/abs/1809.06226) we need to define :\n",
    "\n",
    "- A sequence of encoding convolutional blocks which are to be concatenated. The original architecture consists of 5 dilated (conv + instanceNorm + LeakyRelu) block\n",
    "\n",
    "    - Conv parameters :\n",
    "        - kernel size : 3x3\n",
    "        - Number filters : 32 - 64 - 128 - 32 - 32\n",
    "        - Dilation rate : 1 - 1 - 2 - 3 - 5\n",
    "    - LeakyRelu : `alpha` = 0.3\n",
    "    \n",
    "- A Squeeze Excitation block for the deformable decoder\n",
    "- A sequence of decoding convolutional blocks for deformable registration. The original architecture consists of 5 non-dilated (conv + instanceNorm + LeakyRelu) blocks \n",
    "    - Conv parameters:\n",
    "        - kernel size : 3x3\n",
    "        - Number filters : 128 - 64 - 32 - 32 - 32\n",
    "    - LeakyRelu : `alpha` = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the core blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block_kwgs = {\n",
    "                    \"activation\": \"LeakyReLU\",\n",
    "                    \"activation_kwargs\":{\n",
    "                        \"alpha\": 0.3\n",
    "                        },\n",
    "                    \"normalize\": True,\n",
    "                    \"conv_kwargs\": {\n",
    "                      \"kernel_size\": 3,\n",
    "                      \"padding\": \"same\",\n",
    "                      \"kernel_initializer\": initializers.RandomNormal(mean=0.0, stddev=1e-5)\n",
    "                      }\n",
    "                    }\n",
    "squeeze_ratio = 16\n",
    "conv_block = blocks.ConvBlock(**conv_block_kwgs)\n",
    "squeeze_block = blocks.SqueezeExciteBlock(ratio=squeeze_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and decoding sequences of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_filters = [32, 64, 128, 32, 32]\n",
    "enc_dilation = [(1, 1, 1), (1, 1, 1), (2, 2, 2), (3, 3, 3), (5, 5, 5)]\n",
    "enc_params = [{\"filters\": n_filter, \"dilation_rate\": dil_rate} for (n_filter, dil_rate) in zip(enc_filters, enc_dilation)]\n",
    "\n",
    "dec_filters = [128, 64, 32, 32, 32]\n",
    "dec_params = [{\"filters\": n_filter} for n_filter in dec_filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the original paper adds an ultimate regularized convolution blocks at the end of the linear and deformable decoders. The deformable one consists of a (conv + sigmoid) block with 3 filters and the linear one of a (conv + linear) block with 12 filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_flow_nf = 3\n",
    "lin_flow_nf = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer linear_flow: expected ndim=5, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-804e255875cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mdef_flow_nf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      lin_flow_nf)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarianet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/thera_reg_oma/src/networks/MariaNet.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear_flow'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                  \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandomNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                                  kernel_regularizer=l1(1e-5))(x_lin)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Wrap the source with the flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/main/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer linear_flow: expected ndim=5, found ndim=2"
     ]
    }
   ],
   "source": [
    "marianet = MariaNet(input_shape,\n",
    "                    enc_params,\n",
    "                     dec_params,\n",
    "                     conv_block,\n",
    "                     squeeze_block,\n",
    "                     def_flow_nf,\n",
    "                     lin_flow_nf)\n",
    "model = marianet.build()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"linear_flow\").output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"diffeomorphic_transformer3d_1\").input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training environnements is designed as :\n",
    "\n",
    "```\n",
    "bin\n",
    "|__session_name\n",
    "   |__builder.pickle\n",
    "   |__config.pickle\n",
    "   |__training_history.json\n",
    "   |__checkpoints\n",
    "   |  |__chkpt_{epoch}.h5\n",
    "   |__tensorboard\n",
    "      |__tensorboard log files\n",
    "```\n",
    "\n",
    "where : \n",
    "\n",
    "- `builder.pickle` is a serialized file containing all needed information about the neural network used in the session. It allows to instantiate an object of class `MariaNet`\n",
    "- `config.pickle` is a serialized file containing all needed information about training performed in the session except model architecture. It allows to instantiate an object of class `ConfigFile`\n",
    "- `training_history.json` is a record of losses and metrics evolution for training and validation set, dumped when training is completed\n",
    "- `checkpoints` is a directory containing model weights checkpoints for different epochs\n",
    "- `tensorboard` is a directory containing tensorboard log files\n",
    "\n",
    "The latter directory is created as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"sandbox_session\"\n",
    "config = ConfigFile(session_name)\n",
    "config.setup_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already save the neural net in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_path = os.path.join(config.session_dir, \"builder.pickle\")\n",
    "marianet.serialize(builder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is that all the training configurations except for the model and dataset should be attributes of a `ConfigFile` which allows us to store all those configurations under a single `pickle` serialized file. In the following we will go through the several training configurations and iteratively set them as attributes of `config`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input shape :__\n",
    "\n",
    "This is not properly a training configuration, as it was done previously for the model we need to set the shape toward the inputs need to be resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_input_shape(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Losses :__\n",
    "\n",
    "We propose to use a loss defined by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\|R-\\mathcal{W}(S, G)\\|^2_2 - \\alpha\\, CC(T, R) + \\beta \\|A-A_I\\|_1 + \\gamma\\|\\Phi - \\Phi_I\\|_1\n",
    "$$\n",
    "\n",
    "where $R$ is the predicted moving image, $S$ the source image, $T$ the target image, $G$ the deformation field, $\\mathcal{W}$ the neural net, $CC$ the cross-correlation, $A$ the affine transformation, $A_I$ the identity affine transformation, $\\Phi$ the spatial gradient and $\\Phi_I$ the spatial gradient od the identity transformation.\n",
    "\n",
    "\n",
    "The network's output being of the shape `[deformed, deformable_grad_flow, linear_flow]`, we will define the losses and their weights accordingly. $\\beta$ and $\\gamma$ are set to $10^{-6}$. In practice, $A_I$ and $\\Phi_I$ are represented by a three dimensional array with 3 channels (one by deformation axis), we will hence embedd them as null arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def registration_loss(vol1, vol2):\n",
    "    return klosses.mean_squared_error(vol1, vol2) - 1. * metrics.cross_correlation(vol1, vol2)\n",
    "\n",
    "\n",
    "losses = [registration_loss, klosses.mean_absolute_error, klosses.mean_absolute_error]\n",
    "loss_weights = [1., 1e-5, 1e-5]\n",
    "\n",
    "config.set_losses(losses)\n",
    "config.set_loss_weights(loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimizer :__\n",
    "\n",
    "We choose an Adam optimizer with initial learning rate of $10^{-3}$ and decay $10^{-6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=1e-2, decay=1e-5)\n",
    "config.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Callbacks :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = os.path.join(config.session_dir, ConfigFile.checkpoints_dirname)\n",
    "tensorboard_dir = os.path.join(config.session_dir, ConfigFile.tensorboard_dirname)\n",
    "\n",
    "\n",
    "save_callback = kcallbacks.ModelCheckpoint(os.path.join(checkpoints_dir, ConfigFile.checkpoints_format), \n",
    "                                           verbose=1, \n",
    "                                           save_best_only=True)\n",
    "early_stopping = kcallbacks.EarlyStopping(monitor='val_loss',\n",
    "                                          min_delta=1e-3,\n",
    "                                          patience=20,\n",
    "                                          mode='auto')\n",
    "tbCallBack = kcallbacks.TensorBoard(log_dir=tensorboard_dir, \n",
    "                                    histogram_freq=0, \n",
    "                                    write_graph=True, \n",
    "                                    write_images=True)\n",
    "\n",
    "callbacks = [save_callback, early_stopping, tbCallBack]\n",
    "config.set_callbacks(callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metrics :__ (TO BE ADDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No metrics involved so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training Scope :__\n",
    "\n",
    "Number of training epochs and steps per epoch also have to be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "steps_per_epoch = 50\n",
    "\n",
    "config.set_epochs(epochs)\n",
    "config.set_steps_per_epoch(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dumping configuration :__\n",
    "\n",
    "We can now finally dump the so told serialized file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and training configuration have been defined, we can now combine both to actually perform the training. To do so by initiating an `LunaTrainer` instance with the previously defined objects. But first we need to setup the device we are going to perform the training on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "gpu = '/gpu:' + str(gpu_id)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "set_session(tf.Session(config=tf_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LunaTrainer(model=model,\n",
    "                      device=gpu,\n",
    "                      config_path=os.path.join(config.session_dir, ConfigFile.pickle_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a list of training and validation scans by splitting the Luna dataset with 80/20 ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = loader.get_scan_ids()\n",
    "train_ids, val_ids = train_test_split(all_ids, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training.\n",
    "\n",
    "__Warning__ : this is just a demonstration, you better not run the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
