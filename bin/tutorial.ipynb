{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luna Training Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook is meant to explicit every step of a training session setup and performance assessment. However, in practice it is suitable not to gather all of those steps in a single notebook.__\n",
    "\n",
    "1) Building the network\n",
    "\n",
    "2) Setting up training environement and parameters\n",
    "\n",
    "3) Train the model\n",
    "\n",
    "4) Evaluate\n",
    "\n",
    "In the following, we demonstrate these steps based on what was proposed by [Christodoulis et al. 2018](https://arxiv.org/abs/1809.06226)\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session, get_session\n",
    "import keras.losses as klosses\n",
    "import keras.optimizers as koptimizers\n",
    "import keras.callbacks as kcallbacks\n",
    "import keras.initializers as initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and setup local dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 0 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 1 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 2 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 3 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 4 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 5 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 6 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 7 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n",
      "/Users/dieze/Documents/OMA/TheraPanacea/thera_reg_oma/utils/LungsLoader.py:32: UserWarning: Subset folder 9 not found in data.\n",
      "  warnings.warn(f\"Subset folder {subset_f[-1:]} not found in data.\")\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "base_dir = os.path.dirname(cur_dir)\n",
    "\n",
    "import sys\n",
    "sys.path.append(base_dir)\n",
    "from utils.LungsLoader import LungsLoader\n",
    "from utils.ScanHandler import ScanHandler\n",
    "\n",
    "from src.networks.networks_utils import blocks\n",
    "from src.networks.MariaNet import MariaNet\n",
    "from src.training.config_file import ConfigFile\n",
    "from src.training.luna_training import LunaTrainer\n",
    "from src.evaluation.luna_testing import LunaTester\n",
    "import src.metrics as metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "loader = LungsLoader()\n",
    "handler = ScanHandler(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First hyperparameter showing up would be the input size which is here set to 256x256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the network defined by [Christodoulis et al. 2018](https://arxiv.org/abs/1809.06226) we need to define :\n",
    "\n",
    "- A sequence of encoding convolutional blocks which are to be concatenated. The original architecture consists of 5 dilated (conv + instanceNorm + LeakyRelu) block\n",
    "\n",
    "    - Conv parameters :\n",
    "        - kernel size : 3x3\n",
    "        - Number filters : 32 - 64 - 128 - 32 - 32\n",
    "        - Dilation rate : 1 - 1 - 2 - 3 - 5\n",
    "    - LeakyRelu : `alpha` = 0.3\n",
    "    \n",
    "- A Squeeze Excitation block for the deformable decoder\n",
    "- A sequence of decoding convolutional blocks for deformable registration. The original architecture consists of 5 non-dilated (conv + instanceNorm + LeakyRelu) blocks \n",
    "    - Conv parameters:\n",
    "        - kernel size : 3x3\n",
    "        - Number filters : 128 - 64 - 32 - 32 - 32\n",
    "    - LeakyRelu : `alpha` = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the core blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block_kwgs = {\n",
    "                    \"activation\": \"LeakyReLU\",\n",
    "                    \"activation_kwargs\":{\n",
    "                        \"alpha\": 0.3\n",
    "                        },\n",
    "                    \"normalize\": True,\n",
    "                    \"conv_kwargs\": {\n",
    "                      \"kernel_size\": 3,\n",
    "                      \"padding\": \"same\",\n",
    "                      \"kernel_initializer\": initializers.RandomNormal(mean=0.0, stddev=1e-5)\n",
    "                      }\n",
    "                    }\n",
    "squeeze_ratio = 16\n",
    "conv_block = blocks.ConvBlock(**conv_block_kwgs)\n",
    "squeeze_block = blocks.SqueezeExciteBlock(ratio=squeeze_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and decoding sequences of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_filters = [32, 64, 128, 32, 32]\n",
    "enc_dilation = [(1, 1, 1), (1, 1, 1), (2, 2, 2), (3, 3, 3), (5, 5, 5)]\n",
    "enc_params = [{\"filters\": n_filter, \"dilation_rate\": dil_rate} for (n_filter, dil_rate) in zip(enc_filters, enc_dilation)]\n",
    "\n",
    "dec_filters = [128, 64, 32, 32, 32]\n",
    "dec_params = [{\"filters\": n_filter} for n_filter in dec_filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the original paper adds an ultimate regularized convolution blocks at the end of the linear and deformable decoders. The deformable one consists of a (conv + sigmoid) block with 3 filters and the linear one of a (conv + linear) block with 12 filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_flow_nf = 3\n",
    "lin_flow_nf = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marianet = MariaNet(input_shape,\n",
    "                    enc_params,\n",
    "                     dec_params,\n",
    "                     conv_block,\n",
    "                     squeeze_block,\n",
    "                     def_flow_nf,\n",
    "                     lin_flow_nf)\n",
    "model = marianet.build()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training environnements is designed as :\n",
    "\n",
    "```\n",
    "bin\n",
    "|__session_name\n",
    "   |__builder.pickle\n",
    "   |__config.pickle\n",
    "   |__training_history.json\n",
    "   |__checkpoints\n",
    "   |  |__chkpt_{epoch}.h5\n",
    "   |__tensorboard\n",
    "      |__tensorboard log files\n",
    "```\n",
    "\n",
    "where : \n",
    "\n",
    "- `builder.pickle` is a serialized file containing all needed information about the neural network used in the session. It allows to instantiate an object of class `MariaNet`\n",
    "- `config.pickle` is a serialized file containing all needed information about training performed in the session except model architecture. It allows to instantiate an object of class `ConfigFile`\n",
    "- `training_history.json` is a record of losses and metrics evolution for training and validation set, dumped when training is completed\n",
    "- `checkpoints` is a directory containing model weights checkpoints for different epochs\n",
    "- `tensorboard` is a directory containing tensorboard log files\n",
    "\n",
    "The latter directory is created as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"sandbox_session\"\n",
    "config = ConfigFile(session_name)\n",
    "config.setup_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already save the neural net in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_path = os.path.join(config.session_dir, \"builder.pickle\")\n",
    "marianet.serialize(builder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is that all the training configurations except for the model and dataset should be attributes of a `ConfigFile` which allows us to store all those configurations under a single `pickle` serialized file. In the following we will go through the several training configurations and iteratively set them as attributes of `config`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input shape :__\n",
    "\n",
    "This is not properly a training configuration, as it was done previously for the model we need to set the shape toward the inputs need to be resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_input_shape(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Losses :__\n",
    "\n",
    "We propose to use a loss defined by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\|R-\\mathcal{W}(S, G)\\|^2_2 - \\alpha\\, CC(T, R) + \\beta \\|A-A_I\\|_1 + \\gamma\\|\\Phi - \\Phi_I\\|_1\n",
    "$$\n",
    "\n",
    "where $R$ is the predicted moving image, $S$ the source image, $T$ the target image, $G$ the deformation field, $\\mathcal{W}$ the neural net, $CC$ the cross-correlation, $A$ the affine transformation, $A_I$ the identity affine transformation, $\\Phi$ the spatial gradient and $\\Phi_I$ the spatial gradient od the identity transformation.\n",
    "\n",
    "\n",
    "The network's output being of the shape `[deformed, deformable_grad_flow, linear_flow]`, we will define the losses and their weights accordingly. $\\beta$ and $\\gamma$ are set to $10^{-6}$. In practice, $\\Phi$ is represented by a three dimensional array with 3 channels (one by deformation axis), we will hence embedd $\\Phi_I$ as null array. $A$ is an affine matrix with 12 elements, $A_I$ would hence be the 3x3 identity to which we stack a column of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def registration_loss(vol1, vol2):\n",
    "    return klosses.mean_squared_error(vol1, vol2) - 1. * metrics.cross_correlation(vol1, vol2)\n",
    "\n",
    "\n",
    "losses = [registration_loss, klosses.mean_absolute_error, klosses.mean_absolute_error]\n",
    "loss_weights = [1., 1e-5, 1e-5]\n",
    "\n",
    "config.set_losses(losses)\n",
    "config.set_loss_weights(loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimizer :__\n",
    "\n",
    "We choose an Adam optimizer with initial learning rate of $10^{-3}$ and decay $10^{-6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = koptimizers.Adam(lr=1e-2, decay=1e-5)\n",
    "config.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Callbacks :__\n",
    "\n",
    "_No tensorboard callback should be initialized here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = os.path.join(config.session_dir, ConfigFile.checkpoints_dirname)\n",
    "\n",
    "save_callback = kcallbacks.ModelCheckpoint(os.path.join(checkpoints_dir, ConfigFile.checkpoints_format), \n",
    "                                           verbose=1, \n",
    "                                           save_best_only=True)\n",
    "early_stopping = kcallbacks.EarlyStopping(monitor='val_loss',\n",
    "                                          min_delta=1e-3,\n",
    "                                          patience=20,\n",
    "                                          mode='auto')\n",
    "\n",
    "callbacks = [save_callback, early_stopping]\n",
    "config.set_callbacks(callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metrics :__ (TO BE ADDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No metrics involved so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training Scope :__\n",
    "\n",
    "Number of training epochs and steps per epoch also have to be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "steps_per_epoch = 50\n",
    "\n",
    "config.set_epochs(epochs)\n",
    "config.set_steps_per_epoch(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dumping configuration :__\n",
    "\n",
    "We can now finally dump the so told serialized file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and training configuration have been defined, we can now combine both to actually perform the training. To do so by initiating an `LunaTrainer` instance with the previously defined objects. But first we need to setup the device we are going to perform the training on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "gpu = '/gpu:' + str(gpu_id)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "set_session(tf.Session(config=tf_config))\n",
    "get_session().run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LunaTrainer(model=model,\n",
    "                      device=gpu,\n",
    "                      config_path=os.path.join(config.session_dir, ConfigFile.pickle_filename),\n",
    "                      tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a list of training, validation and testing scans by splitting the Luna dataset with 80/10/10 ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = loader.get_scan_ids()\n",
    "train_ids, val_ids = train_test_split(all_ids, test_size=0.2)\n",
    "val_ids, test_ids = train_test_split(val_ids, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training.\n",
    "\n",
    "__Warning__ : this is just a demonstration, you better not run the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained, or even during training, it's important to be able to assess the model performances. To measure the latter, a set of metrics and of testing scans must be defined.\n",
    "\n",
    "- Registration metrics : MSE, Cross Correlation\n",
    "- Segmentation metrics : Dice Score, Haussdorf Distance, Mutual Information\n",
    "\n",
    "Similarly to what has been done for training we will instantiate a `LunaTester` object that will handle testing scores computation. Then getting insights from these scores is up to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = os.path.join(config.session_dir, ConfigFile.checkpoints_dirname, \"chkpt_01.h5\")\n",
    "\n",
    "tester = LunaTester(model=model,\n",
    "                    metric_dict=LunaTester.reg_metrics,\n",
    "                    config_path=os.path.join(config.session_dir, ConfigFile.pickle_filename),\n",
    "                    weights_path=weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of testing scans : 9\n",
      "\n",
      "********** Beginning evaluation **********\n",
      "\n",
      "Evaluated 0/9 scans\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse\n",
      "<function mse at 0x1a3298fd90>\n",
      "cross_correlation\n",
      "<function cross_correlation at 0x1a3298fbf8>\n",
      "{'mse': [6.058916010084032e-06], 'cross_correlation': [0.0009826254442117112]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluated 1/9 scans\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse\n",
      "<function mse at 0x1a3298fd90>\n",
      "cross_correlation\n",
      "<function cross_correlation at 0x1a3298fbf8>\n",
      "{'mse': [4.863648258723169e-06], 'cross_correlation': [1.3915944003120343e-08]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluated 2/9 scans\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse\n",
      "<function mse at 0x1a3298fd90>\n",
      "cross_correlation\n",
      "<function cross_correlation at 0x1a3298fbf8>\n",
      "{'mse': [9.590188469654259e-06], 'cross_correlation': [0.00016219653561637585]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluated 3/9 scans\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse\n",
      "<function mse at 0x1a3298fd90>\n",
      "cross_correlation\n",
      "<function cross_correlation at 0x1a3298fbf8>\n",
      "{'mse': [4.884045666122784e-06], 'cross_correlation': [1.1973149826401037e-10]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>cross_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>9.826254e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.391594e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.621965e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.197315e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mse  cross_correlation\n",
       "0  0.000006       9.826254e-04\n",
       "1  0.000005       1.391594e-08\n",
       "2  0.000010       1.621965e-04\n",
       "3  0.000005       1.197315e-10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = tester.evaluate(test_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
